---Create commands :
kubectl run nginx --image=nginx
kubectl run nginx --image=nginx --dry-run=client -o yaml > nginx_pod.yaml
kubectl create deployment nginx --image=nginx -l=name=nginx --replicas=1
kubectl create namespace dev
k create configmap <config-name> --from-literal=<key>=<value>
kubectl create configmap ingress-nginx-controller --namespace ingress-nginx
create service account: k create serviceaccount ingress-nginx -n ingress-nginx

---Minikube start:
sudo service docker status
sudo service --status-all
sudo usermod -aG docker chaldan && newgrp docker
minikube start --driver=docker
sudo usermod -a -G docker chaldan
docker:x:998:chaldan
newgrp docker
minikube dashboard --url
minikube service nginx-service --url

---Commands:
k get po mysql -n epsilon -o yaml > mysql.yaml
k logs podname -f --previous
k get pod pvc-user -o yaml | kubectl replace -f - --force
k exec pvc-user -- touch /mnt/share/my-pvc/coolfile
kubectl set image deployment/my-deployment mycontainer=myimage
k describe svc pod-hello | grep -i selector
k describe pod pod-hello | grep -i ip:
minikube ssh
k expose deployment nginx --name=nginx-service --port=80 --type=NodePort
curl http:///<ipofnode>:<nodePort>
curl <pod-ip>
k taint nodes node01 spray=mortein:NoSchedule
k taint nodes controlplane node-role.kubernetes.io=control-plane:NoSchedule-
k get po -L redis
k edit svc pod-hello
k config set-context ${kubectl config current-context} --namespace=dev
k get pods --all-namespaces
k get cs or k get pods -n kube-system
k replace --force -f nginx.yaml
k get all -l env=prod
k config get-clusters -> get cluster names 
k config view -> view complet clusters info 
to limit resources in a namespace, create a resource quota
to manually schedule a pod, add noName: nodename into yaml on the same line as containers or create kind:Binding 
k scale --replicas=6 replicaset myapp-replicaset
k get pod busybox -ojson|jq '.spec.containers[0].image' "busybox"
kubectl api-resources         # List resource types
kubectl explain <resource>    # Show information about a resource
kubectl explain deployment
# List resources in cluster
kubectl get <resource>                    # In current namespace
kubectl get <resource> -n <namespace>     # In specific namespace
kubectl get <resource> --all-namespaces   # In all namespaces
kubectl get <resource> -o wide            # Add extended information
kubectl get <resource> -o yaml            # output in YAML format
kubectl get <resource> -o json            # output in JSON format
# Example
kubectl get pods [-n abc|--all-namespaces] [-o wide|yaml|json]
kubectl config --kubeconfig=/root/my-kube-config use-context research
kubectl config --kubeconfig=/root/my-kube-config current-context

---Aliases:
alias e='./etcdctl'
alias k=kubectl
alias kgp="k get pod"
alias kgd="k get deploy"
alias kgs="k get svc"
alias kgn="k get nodes"
alias kgrs="k get rs"
alias kd="k describe"
alias kge="k get events --sort-by='.metadata.creationTimestamp' |tail -8"
export do="--dry-run=client -o yaml"
export now="--force --grace-period 0"
k delete pod test $now
kubectl help run
kubectl -it  run busybox --rm --image=busybox -- sh 
/ # wget -O- 172.17.254.255
crictl ps -a
crictl logs container-id
top
df -h
service kubelet status
sudo journalctl -u kubelet
ssh node01 "service kubelet start" sau systemctl restart kubelet

----

etcd
ETCD_VER=v3.4.23

rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz
rm -rf /tmp/etcd-download-test && mkdir -p /tmp/etcd-download-test

DOWNLOAD_URL=https://github.com/etcd-io/etcd/releases/download
curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz
tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1
rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz

/tmp/etcd-download-test/etcd --version
/tmp/etcd-download-test/etcdctl version
---


ETCD backup
ps -ef | grep etcd

ETCDCTL_API=3 etcdctl snapshot save /home/chaldan/snapshot-pre-boot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/var/lib/minikube/certs/etcd/ca.crt \
  --cert=/var/lib/minikube/certs/etcd/server.crt \
  --key=/var/lib/minikube/certs/etcd/server.key 



Node selector:
	k label nodes <node-name> <label-key>=<label-value>
	after that, place a nodeSelector: size: Large in the pod under spec
Node affinity: to ensure that pods are scheduled on different nodes:
	in pod set affinity under spec and use matchExpressions, size NotInSmall/Exists. 2 types of affinity types: requiredDurring...preferredDuring
	
Resource requirements and limits in pod
	under spec: resources : requests memory and cpu and limits memory and cpu
static pods: check the config file

ps -aux | grep kubelet -> config file path

minikube addons enable metrics-server
k top nodes 
k top po

k logs -f <pod-name> <container-name> daca is mai multe containere

k set image deployment/mydeploy nginx=nginx:1.19
k rollout status deployment/my-deployment
kodekloud/webapp-color:v1

k drain node-01 -> moves the nodes from that pod to another node. (recreates them). the node comes back online unschedulable and we need to uncordon.
k cordon node-01 -> marks node unschedulable, dont accepts new pods. 
k uncordon node node-01


cluster upgrade:
k drain controlplane
k get nodes -o wide check cluster version
apt-get update && apt-get install kubeadm=1.26.0-00
kubeadm version
kubeadm upgrade plan
kubeadm upgrade apply v1.26.0
apt-get update && apt-get install kubelet=1.26.0-00
systemctl daemon-reload
systemctl restart kubelet
k uncordon controlplan

worker node upgrade: 
k drain node01
ssh node01
apt-get update && apt-get install kubeadm=1.26.0-00
kubeadm upgrade node
apt-get update && apt-get install kubelet=1.26.0-00
k uncordon node01

ETCD backup:
1. check ETCD version: 
	k logs etcd-controlplane -n kube-system | grep -i etcd-version:
	or k describe pod etcd-controlplane -n kube-system | grep -i image:
kubectl logs etcd-controlplane -n kube-system or kubectl describe pod etcd-controlplane -n kube-system

ETCDCTL_API=3 etcdctl snapshot save /opt/etcd-backup.db \
--endpoints=https://192.22.58.3:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \	
--key=/etc/kubernetes/pki/etcd/server.key								
view status of backup: ETCDCTL_API=3 etcdctl snapshot status /opt/etcd-backup.db
	
ETCD restore:
service kube-apiserver stop
ETCDCTL_API=3 etcdctl snapshot restore /opt/snapshot-pre-boot.db --data-dir /var/lib/etcd-from-backup
change --data-dir=/var/lib/etcd-from-backup -> cd /etc/kubernetes/manifests -> change --data-dir=/var/lib/etcd-from-backup
change also the volumeMounts: mountPath: /var/lib/etcd-from-backup
change also the hostPath path to /var/lib/etcd-from-backup 
watch "crictl ps | grep etcd" to see when etcd is back up
systemctl daemon-reload
service etcd restart
service kube-apiserver start
-----


ETCDCTL_API=3 etcdctl snapshot save /opt/etcd-backup.db --endpoints=https://192.22.58.3:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key	
ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshotdb

cat /etc/kubernetes/manifests/etcd.yaml | grep file

---
Roles and access
1. create csr and get approved
2. create role with priviledges
3. create rolebinding to bind role to user john
create a CSR kind yaml
cat john.csr | base64 | tr -d "\n" -> copy to yaml
k create -f yaml
csr in pending
k certificate approve john-developer
k create role --help
k auth --help
k auth can-i get pods --namespace=development --as john
k create rolebinding --help
k create rolebinding john-developer --role=developer --user=john --namespace=development
k auth can-i get pods --namespace=development --as john


---
verify user and group ID kubectl exec -it non-root-pod -- id

Access to context kubeconfig
k config view
kubectl config set-credentials martin --client-certificate=/root/martin.crt --client-key=/root/martin.key --namespace=development
kubectl config set-context developer --cluster=kubernetes --namespace=development --user=martin
kubectl create role developer-role --verb=get,list,watch --resource=pods -n development
k create rolebinding developer-rolebinding --role=developer-role --user=martin --namespace=development
kubectl config use-context kubernetes-admin@kubernetes
---
Networking
find the ip address of controlplane -> k get po -n kube-system -o wide
network interface configured for cluster connectivity on the controlplane node -> ip a | grep -B2 <ip> which results to eth0
What is the MAC address of the interface on the controlplane node -> ip link show eth0 sau ifconfig eth0
ip address of node01 -> k get no  -o wide 
MAC address of node01-> ssh node01 -> ip a | grep -B2 <ip> sau ssh node01 ifconfig eth0
show all network interfaces -> ifconfig -a
We use Containerd as our container runtime. What is the interface/bridge created by Containerd on the controlplane node? ifconfig -a 
If you were to ping google from the controlplane node, which route does it take? ip r sau ip route si vezi default route
what is the port of kube-scheduler -> k describe po kube-scheduler -n kube-system sau netstat -natulp | grep kube-scheduler
what is the port of ETCD -> netstat -natulp | grep etcd | grep LISTEN
CNi e configurat in kubelet -> ps aux | grep kubelet -> check folders bin and conf-> ipam subnet arata ip urilela poduri
what is the path configured with all binaries of CNI supported plugins? /opt/cni/bin
What network range are the nodes in the cluster part of? -> ip addr
What is the range of IP addresses configured for PODs on this cluster? -> k logs <weave-pod-ip> -n kube-system | grep -i ipalloc-range
What is the IP Range configured for the services within the cluster? cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range
What type of proxy is the kube-proxy configured to use? k logs kube-proxy-ckcl6 -n kube-system
How does this Kubernetes cluster ensure that a kube-proxy pod runs on all nodes in the cluster?
coreDns -> </weave-pod-ip> cat /etc/coredns/Corefile
k get configmap -n kube-system
k get service -n kube0system -> check kube-dns ip
service ip of the coredns -> cat /etc/resolv/conf
host web-service to identify ip address of svc

---
RBAC
check authorization of apiserver -> k describe pod kube-apiserver-controlplane -n kube-system
check roles -> k get roles -A
check the role -> k describe role kube-proxy -n kube-system
check to which account is the role assigned -> k get rolebindings -n kube-system -> k describe rolebinding kube-proxy -n kube-system
k auth can-i get pods --as dev-user = k get pods --as dev-user
create role -> kubectl create role developer --verb=create,list,delete --resource=pods -o yaml > developer.yaml
create rolebinding -> kubectl create rolebinding dev-user-binding --user=dev-user --role=developer

---
Ingress
k get ingress -A
deploy ingress controller:
1. create ns -> k create ns ingress-nginx
2. create configmap: kubectl create configmap ingress-nginx-controller --namespace ingress-nginx
3. create service account: k create serviceaccount ingress-nginx -n ingress-nginx

---
Json
k get pods -o=jsonpath='{.items[0].spec.containers[0].image}'
k get nodes -o=jsonpath='{.items[*].metadata.name} {"\n"} {.items[*].status.capacity.cpu}'
Use JSON PATH query to fetch node names and store them in /opt/outputs/node_names.txt   k get no -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt
Use JSON PATH query to retrieve the osImages of all the nodes and store it in a file /opt/outputs/nodes_os.txt. k get no -o=jsonpath='{.items[*].status.nodeInfo.osImage}' > /opt/outputs/node_os.txt
A set of Persistent Volumes are available. Sort them based on their capacity and store the result in the file /opt/outputs/storage-capacity-sorted.txt.
k get pv --sort-by=.spec.capacity.storage
kubectl get pv --sort-by=.spec.capacity.storage -o=custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage > /opt/outputs/pv-and-capacity-sorted.txt
kubectl config view --kubeconfig=/root/CKA/admin.kubeconfig
k get deploy -n admin2406 -o=custom-columns=DEPLOYMENT:.metadata.name,CONTAINER_IMAGE:.spec.template.spec.containers[0].image,READY_REPLICAS:.status.readyReplicas,NAMESPACE:.metadata.namespace --sort-by=.metadata.name > /opt/admin2406_data